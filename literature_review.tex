\section{Introduction}
The research topic focuses on parallel computing to optimize algorithms for both desktop and embedded CPUs. Parallel computing is widely regarded as a complex area within engineering and computer science. To develop or enhance multi-threaded solutions, a thorough understanding of the underlying algorithms and the specific goals of the application is crucial. This foundational knowledge is essential before attempting any modifications to ensure that changes do not deviate from the application’s original objectives. In our project, we concentrate on three applications: \texttt{mpbenchmark}, \texttt{MobileNet}, and \texttt{DeBaTE-FI} platform. It is imperative to understand the background and functionality of these applications by utilising the existing publications centred around them. 

Another goal of the literature review is to identify existing solutions to avoid duplicating efforts. Adherence to programming best practices is also a critical aspect of the project, given the complexity of parallel computing. This project strives to ensure that all developed software solutions conform to the best practices and guidelines recommended by industry experts, necessitating a comprehensive literature review.

The literature review is structured to support the report's overall organization, with individual subsections dedicated to the three main objectives, the review covers providing background information, reviewing current research, and offering critical analysis. The review then explores build systems, focusing on how this project aims to develop cross-platform applications that can operate on different operating systems such as \texttt{Linux}, \texttt{Windows}, or \texttt{macOS}. Further research is directed towards software design and multi-threading in line with modern \texttt{C++} guidelines, including discussions on relevant libraries, tools, and research on parallel algorithms. The conclusion section summarizes the findings from the preceding sections and underscores the significance of the reviewed literature to the research question.

\section{Objective 1: \texttt{mpbenchmark}}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

% Paper 1
Our project initially focuses on the \texttt{JetBench} software, featured in a paper from the ARCS conference series, a prominent event in Germany with over 30 years of history in computer architecture and operating systems research. The paper, ``JetBench: An Open Source Real-Time Multiprocessor Benchmark" by Muhammad Yasir Qadri, Dorian Matichard, and Klaus D. McDonald-Maier, was published in a 2010 conference book\cite{JetBench_paper}. It is pivotal for our research as it introduces a benchmark tool that evaluates real-time multiprocessor performance, aligning with our project objectives.

\texttt{JetBench} responds to the lack of real-time, multi-threaded benchmarks by simulating thermodynamic jet engine calculations. It primarily tests multi-core CPU capabilities through ALU-centric operations, including arithmetic and mathematical function computations, with an 88.6\% parallel operation rate. Developed in \texttt{C} and utilising \texttt{OpenMP} for multi-threading, \texttt{JetBench} emphasizes portability and scalability across different computing systems without relying on system-specific libraries.

The benchmark's limitations include its narrow computation scope to ensure compatibility with lower-end CPUs and the exclusion of I/O operations to boost portability. Despite these limitations, \texttt{JetBench}'s design effectively evaluates multi-core performance across diverse systems, making it an excellent tool for our project aimed at enhancing multi-threading on both desktop and embedded platforms.

% Paper 2

To address the identified limitations of the \texttt{JetBench} software, a novel solution was devised using diverse programming languages. This innovation was detailed in a publication titled ``Using JetBench to Evaluate the Efficiency of Multiprocessor Support for Parallel Processing,'' authored by HaiTao Mei and Andy Wellings. The paper, released in 2014, was part of the proceedings of a notable conference\cite{mpbenchmark_paper}.

In their research, the authors of \texttt{mpbenchmark}\cite{mpbenchmark_paper} adopted an object-oriented programming (OOP) approach for \texttt{C\#} and \texttt{Java} implementations. They standardised result collection by executing the application 30 times with a varying number of threads, employing a specific Linux command to control CPU core usage. Results were collected using a desktop CPU and \texttt{Simics}(a virtual platform to simulate a high end 128-core CPU).  This methodology is in line with standard practices for benchmark data collection.

The authors of \texttt{mpbenchmark}\cite{mpbenchmark_paper} highlighted several design flaws in the original \texttt{JetBench} software\cite{JetBench_paper}, including the excessive use of shared variables leading to inaccurate performance results, race conditions from variables shared across threads, and erroneous benchmark output data printing. To remedy these issues, they restructured the \texttt{JetBench} code and introduced implementations in \texttt{Ada}, \texttt{C\#}, and \texttt{Java}. Notably, the compiled languages (\texttt{C} and \texttt{Ada}) demonstrated superior performance. Additionally, the impact of virtual cores enabled by simultaneous multi-threading (SMT) was observed to vary inconsistently across different programming languages.

In the second paper\cite{mpbenchmark_paper} several limitations were found. Firstly, the data collection, based on just 30 runs, may be insufficient, especially for applications with minimal execution times on high-end CPUs. Additionally, the research focused solely on desktop CPUs and a system simulator mimicking a high-end 128-core CPU, excluding experiments on embedded or lower-end CPUs. Furthermore, given the superior performance of compiled languages like \texttt{C} and \texttt{Ada}, further research could beneficially explore comparisons with \texttt{C++}, another compiled language often regarded as superior to both \texttt{C} and \texttt{Ada} in certain contexts. This paper lays a solid foundation for our project, where its limitations present opportunities for further investigation, and its methodological approaches offer a model for emulation.

\section{Objective 2: \texttt{MobileNet}}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

\texttt{MobileNet} is a specialized machine learning algorithm that falls under the category of convolutional neural networks (CNNs). Developed specifically for mobile and embedded vision applications, \texttt{MobileNet} is discussed in detail in a 2017 paper by Cornell University titled ``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"\cite{mobilenet_paper}. 

This paper\cite{mobilenet_paper} delves into the mathematical foundations required to construct the \texttt{MobileNet}, which are considered beyond the scope of our project. To summarize, the \texttt{MobileNet} architecture employs depth-wise separable convolutions that enhance memory efficiency and leverage optimized numerical linear algebra algorithms to reduce latency.

\texttt{MobileNet} introduces two crucial hyper-parameters: the width and resolution multiplier. These allow for adjustments to the model's size and computational requirements, with smaller \texttt{MobileNet} models typically showing slightly reduced accuracy compared to their larger counterparts. In comparative analyses with other renowned models like \texttt{GoogleNet}, \texttt{VGG 16}, and \texttt{FaceNet}; \texttt{MobileNet} offers comparable accuracy levels.

Despite its advantages of reduced size and latency, making it ideal for mobile and embedded devices, the paper\cite{mobilenet_paper} does not address the use of parallel computing techniques, presenting an opportunity for further exploration of its source code to enhance performance through parallelism.

For the practical component of our project, a \texttt{C++} implementation of \texttt{MobileNet} was chosen, sourced from a public repository on the \texttt{GitHub}\cite{mobilenet_repo}. This particular implementation was part of a computer science competition\cite{mobilenet_competition}, challenging participants to classify images into 20 predefined classes using provided training, validation, and testing data, with submissions made in a \texttt{.zip} file format.

The \texttt{C++} implementation comprises several header and source files but lacks build software or instructions, which poses a significant initial hurdle. Moreover, the implementation features a method for testing custom images against 12 predefined classes. However, these class names are in Chinese, necessitating translation into English for broader accessibility. Setting up and running the project before making any enhancements will be challenging. Nevertheless, this \texttt{C++} implementation presents a valuable opportunity to apply parallelisation techniques to improve the efficiency of the widely recognized \texttt{MobileNet} algorithm.

\section{Objective 3: \texttt{DeBaTE-FI} platform}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

The \texttt{DeBaTE-FI} platform, developed by PhD student Alex Henneman under the supervision of Dr. Luciano Ost at Loughborough University, is a GUI application detailed in the publication ``DeBaTE-FI: A Debugger-Based Fault Injector Infrastructure for IoT Soft Error Reliability Assessment"\cite{debate_fi_publication}.

\texttt{DeBaTE-FI} was engineered to test embedded devices for soft errors—random errors in processors caused by background radiation, which can be particularly harmful in safety-critical systems. Testing methods for soft errors range from high-level analytical models, which are easier to implement but less accurate, to costly full radiation tests, which offer higher precision. \texttt{DeBaTE-FI} seeks to emulate the latter's accuracy by simulating soft errors in embedded devices. The platform utilized \texttt{Raspberry Pi 4} and \texttt{Rock 4C} to conduct tests on \texttt{STM32F7} microcontrollers (MCUs). The research involved running a neural network binary on these MCUs, measuring the execution time across various testing scenarios, and using different numbers of MCUs to evaluate speed and efficiency. Additionally, the results from \texttt{DeBaTE-FI} were compared with those from simulation software to assess accuracy, and \texttt{OpenOCD} was used for MCU communication.

The authors observed that increasing the number of MCUs reduced overall runtime. They also discovered that while multithreading in Python did not improve performance, multiprocessing—which enables parallel processing—did enhance performance significantly. The study confirmed that \texttt{DeBaTE-FI} provides greater accuracy than the high-level simulation software they aimed to surpass. Challenges arose when using \texttt{Raspberry Pi 4}, particularly with USB controller limitations that restricted the connection to no more than 8 MCUs, prompting a switch to \texttt{Rock 4C}, which supported up to 30 MCUs but also reached full CPU utilization under this load.

Despite the innovative approach of \texttt{DeBaTE-FI}, the platform is hindered by several significant limitations. The primary issue is its lengthy runtime, which can extend to several minutes, suggesting the presence of performance bottlenecks. Moreover, the performance on embedded Linux devices, such as \texttt{Rock 4C}, is modest, highlighting the need for optimization across both desktop and embedded Linux devices (or single-board computers). Additionally, the utilization of \texttt{OpenOCD} for MCU communication presents an opportunity to enhance efficiency and expedite processes. The central goal of our project is to tackle these issues by improving the runtime efficiency and scalability of \texttt{DeBaTE-FI}.

\section{Cross platform build systems}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

% Paper 1
A 2008 paper published by Technical University of Munich titled  ``A Cross Platform Development Workflow for \texttt{C/C++} Application"\cite{cmake_publication_1} underscores the efficacy of \texttt{CMake} in addressing significant challenges in cross-platform project management, particularly those stemming from the constraints of build systems and source code management practices. This is highlighted despite the standardization of \texttt{C} and \texttt{C++} by \texttt{ANSI} and \texttt{ISO}, and the availability of the \texttt{C} library and \texttt{STL}. Given our project's focus on constructing \texttt{C++} applications and the need to automate build processes across multiple operating systems such as Linux, macOS, and Windows, \texttt{CMake} emerges as an suitable choice. 

The authors highlight \texttt{CMake}'s ability to maintain developers' preferred environments across different platforms, reducing the need to synchronize platform-specific project files. By implementing \texttt{CMake} in projects like \texttt{KDE4}, an \texttt{OpenGL} application, and a platform-independent camera interface, the paper advocates for a simplified cross-platform development workflow.

\texttt{CMake} supports the configuration step vital for locating external package headers and libraries, offering numerous pre-written scripts and allowing developers to create tailored scripts for specific needs. It also facilitates the development of \texttt{qiew}, a platform-independent \texttt{VRML} viewer using the \texttt{Qt} toolkit and \texttt{Coin3D}, exemplifying consistent cross-platform user experiences.

Compared to tools like GNU Autotools, \texttt{CMake} has a gentler learning curve, enhancing accessibility and reducing the need for extensive training. Its effectiveness is illustrated at Technische Universität München, where it bridges diverse development environments in vision and robotics research, also adopted by two spin-offs. The tool aids in project generation, synchronization, configuration, dependency resolution, and deployment packaging, as shown in its use in \texttt{KDE4} and \texttt{OpenSceneGraph}.

While outlining \texttt{CMake}'s benefits, the paper lacks a discussion on its potential limitations, such as configuration overheads and handling complex scenarios. Including tool comparisons and considering technology's rapid evolution could provide a more balanced view of \texttt{CMake}'s current relevance in modern software development landscapes.

% Paper 2 
Addressing the previous paper's limitations and potentially out of date content more research was conducted to find more recent publications around the use of build automation software. A recognised publication titled ``Large Scale Software Building with CMake in ATLAS" obtained from the ``Journal of Physics: Conference Series" published in 2017\cite{cmake_publication_2} discusses the use of build automation software for \texttt{C++} application in an industrial context. 

The text discusses \texttt{CMake}'s application in managing complex, multi-platform software projects like ATLAS. It uses a master \texttt{CMakeLists.txt} file and subdirectories reflecting the \texttt{CMT} package structure for streamlined project management. This approach includes \texttt{Atlas}-specific functions, \texttt{add\_subdirectory} for package inclusion, and helper targets for building and testing. It ensures modularity with prefixed libraries and enhances flexibility with scripts for runtime environments and relocatable project files.

Key findings from migrating ATLAS's offline software to \texttt{CMake} highlight improved build efficiency and streamlined processes. The integration of \texttt{CMake} has optimised resource use on build machines and, alongside consolidating projects into a single repository named Athena, reduced build times to about three hours. The transition to \texttt{CMake} and \texttt{Git} aligns ATLAS with standard software development practices and is expected to support \texttt{LHC's Run 3} and beyond.

The shift to \texttt{CMake} is critically evaluated for its strengths in build efficiency and community support, and weaknesses in dependency management and initial build time improvements. While it modernizes ATLAS's build processes, ongoing refinement is essential to maximize \texttt{CMake}'s benefits and ensure long-term scalability.

Conducting research on build automation software for \texttt{C++} projects\cite{cmake_publication_1}\cite{cmake_publication_2} outline the \texttt{CMake} software as a useful tool that is industry standard and can be used for this project. 

\section{\texttt{C++} guidelines and best practices}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

% Paper 1
Multi-threading in C++ is a complex topic therefore in order to adhere to \texttt{C++} core guidelines,  textbook titled ``\texttt{C++} concurrency in Action" written by Anthony William was researched\cite{c++_concurrency_in_action}. This book is regarded by many as the gold standard for multi-threading concepts in modern \texttt{C++}, as discussed by the community on stack overflow\cite{c++_books_stackoverflow}. The second edition of this book has been updated to include features the \texttt{C++17} standard.

The book\cite{c++_concurrency_in_action} thoroughly examines the methodologies for developing concurrent and multithreaded applications with \texttt{C++}. This edition not only reviews the foundational principles from \texttt{C++11} and \texttt{C++14}, but also incorporates advancements from \texttt{C++17}, providing an extensive overview of modern \texttt{C++} concurrency features. Williams methodically explores various strategies, including lock-based and lock-free programming, as well as the development of concurrent data structures and algorithms. Each technique is clearly explained, focusing on practical implementation and the effective use of the language's concurrency capabilities. The book also addresses the challenges of concurrency with detailed examples and solutions, making it an essential resource for developers aiming to utilize the full potential of multicore processing with \texttt{C++}.

Strengths of the book\cite{c++_books_stackoverflow} include comprehensive explanations and practical examples that are accessible even to programmers with a basic understanding of \texttt{C++}, making complex concurrency concepts easier to grasp. Williams meticulously explores both fundamental and advanced concurrency techniques, accompanied by a methodical presentation of best practices and common pitfalls, which helps developers build robust, high-performance applications. However, the book's weaknesses include its dense technical content, potentially overwhelming for beginners without a solid \texttt{C++} background or those new to concurrency. While it thoroughly addresses \texttt{C++17}, those seeking insights into the latest concurrency features of \texttt{C++20} might find the coverage lacking, highlighting a gap in an otherwise excellent resource. This gap points to an area for potential future updates or editions as \texttt{C++} continues to evolve. Overall, the book remains an invaluable resource for both experienced and intermediate \texttt{C++} developers aiming to deepen their understanding of concurrent programming.

% Paper 2
Another valuable study, titled ``Multi-Threaded Parallel I/O for OpenMP Applications," was published in the International Journal of Parallel Programming in 2014\cite{openmp_usage_hpc}. This research is pertinent to our project as various libraries exhibit unique strengths and weaknesses, and selecting the appropriate library is crucial for achieving optimal performance.

The study focuses on optimizing multi-threaded parallel I/O operations within the \texttt{OpenMP} framework using a parallel I/O library integrated into the \texttt{OpenUH} compiler. Techniques include \texttt{omp\_file\_read\_all()} function for collective data reading, merging small I/O requests into larger ones, and splitting large requests among threads to boost efficiency. A contiguity analyser and a work manager optimize data handling and thread utilization, with adaptive techniques accommodating different file system behaviours, including the use of the Parallel Log-structured File System (\texttt{PLFS}) on systems like \texttt{Lustre}.

Findings highlight significant performance improvements in I/O operations, especially in write operations, achieved by enabling multiple user-level threads to operate concurrently. Write speeds approach those of separate file writes per thread, surpassing the one-file-per-thread method without needing a merging step. However, scalability for read operations is limited, showing benefits only up to four threads. The integration of \texttt{OpenMP} I/O with \texttt{MPI/OpenMP} hybrid applications is limited by \texttt{MPI} domain partitioning, though \texttt{OpenMP I/O} could enhance \texttt{MPI I/O} systems like \texttt{ROMIO} and \texttt{OMPIO}.

The paper\cite{openmp_usage_hpc} evaluates parallel I/O interfaces for \texttt{OpenMP}, demonstrating enhanced performance over traditional sequential I/O and discussing interface specifications and optimizations across various file systems and benchmarks. Future research will refine these interfaces for recent \texttt{OpenMP} developments and enhance the parallel I/O library for \texttt{NUMA} architectures, integrating multiple storage resources for broader application in diverse computing environments.

The project aims to emulate the techniques and software design highlighted in the book\cite{c++_concurrency_in_action} to ensure the produced software adheres to the best practices. Another library, \texttt{OpenMP}, which is widely used in high-performance computing and parallel I/O interfaces\cite{openmp_usage_hpc}, offers a straightforward method for implementing multi-threading in both \texttt{C} and \texttt{C++}. This library is particularly useful for our project as it provides an alternative to the \texttt{std::thread} library in \texttt{C++}. 

\section{Literature review conclusion}

Research on the \texttt{JetBench} software has been identified as essential, as documented in the \texttt{mpbenchmark} publication\cite{mpbenchmark_paper}, which tracks its evolution and benchmarks performance across various programming languages. Analysis of both the \texttt{mpbenchmark} and \texttt{JetBench} papers\cite{JetBench_paper}\cite{mpbenchmark_paper} reveals substantial advancements, providing a robust foundation for this project to develop a more optimised \texttt{C++} solution. This initiative, guided by best practices from the renowned \texttt{C++ Concurrency in Action}\cite{c++_concurrency_in_action}—a resource recommended on Stack Overflow\cite{c++_books_stackoverflow}, builds upon existing work without redundancy.

For the second objective, understanding of the \texttt{MobileNet} algorithm was enhanced through its foundational publication\cite{mobilenet_paper}. This project works with a \texttt{C++} implementation of \texttt{MobileNet}, sourced from an open-source GitHub repository\cite{mobilenet_repo}, to analyse and enhance the code through parallelisation. Key resources for this enhancement include the \texttt{C++} concurrency textbook\cite{c++_concurrency_in_action} and various multi-threading libraries\cite{openmp_usage_hpc}, which are crucial for optimizing performance.

The first two objectives necessitate the development of cross-platform \texttt{C++} applications, for which \texttt{CMake} is suitable for this project. Investigations into \texttt{CMake}'s application across multiple platforms have been informed by pertinent literature\cite{cmake_publication_1}\cite{cmake_publication_2}. This research is essential to streamline the build and compilation processes, ensuring they meet industry standards.

The third objective addresses a unique challenge: enhancing the \texttt{DeBaTE-FI} platform, developed by Loughborough University’s PhD students and faculty\cite{debate_fi_publication}. A thorough review of its publication was critical to comprehend its design and functionality. The project will leverage the existing features of \texttt{DeBaTE-FI}, aiming to refine its application and benchmark against the original performance metrics. 


