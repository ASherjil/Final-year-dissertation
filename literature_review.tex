\section{Introduction}
The research topic focuses on parallel computing to optimize algorithms for both desktop and embedded CPUs. Parallel computing is widely regarded as a complex area within engineering and computer science. To develop or enhance multi-threaded solutions, a thorough understanding of the underlying algorithms and the specific goals of the application is crucial. This foundational knowledge is essential before attempting any modifications to ensure that changes do not deviate from the application’s original objectives. In our project, we concentrate on three applications: \texttt{mpbenchmark}, \texttt{MobileNet}, and \texttt{DeBaTE-FI} platform. It is imperative to understand the background and functionality of these applications by utilising the existing publications centred around them. 

Another goal of the literature review is to identify existing solutions to avoid duplicating efforts. Adherence to programming best practices is also a critical aspect of the project, given the complexity of parallel computing. This project strives to ensure that all developed software solutions conform to the best practices and guidelines recommended by industry experts, necessitating a comprehensive literature review.

The literature review is structured to support the report's overall organization, with individual subsections dedicated to the three main objectives, the review covers providing background information, reviewing current research, and offering critical analysis. The review then explores build systems, focusing on how this project aims to develop cross-platform applications that can operate on different operating systems such as \texttt{Linux}, \texttt{Windows}, or \texttt{macOS}. Further research is directed towards software design and multi-threading in line with modern \texttt{C++} guidelines, including discussions on relevant libraries, tools, and research on parallel algorithms. The conclusion section summarizes the findings from the preceding sections and underscores the significance of the reviewed literature to the research question.

\section{Objective 1: \texttt{mpbenchmark}}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

Our initial focus is on the original \texttt{JetBench} software, which was featured in a publication arising from an international conference held annually in Germany. This conference, part of the ARCS (Architecture of Computing Systems) series, boasts a tradition spanning over 30 years and is renowned for presenting cutting-edge research in computer architecture and operating systems. The specific conference relevant to the first objective of this project took place in 2010. Following the conference, a book was published containing a compilation of papers presented at the event. Our project specifically examines the paper titled ``JetBench: An Open Source Real-Time Multiprocessor Benchmark", authored by Muhammad Yasir Qadri, Dorian Matichard, and Klaus D. McDonald-Maier. This paper is of particular interest to our research as it introduces a multiprocessor benchmark software that aligns well with our project's goals, providing a foundational tool for assessing real-time multiprocessor performance\cite{JetBench_paper}.

The \texttt{JetBench} software emerged in response to the observed scarcity of real-time, multi-threaded benchmarks. Designed to simulate the thermodynamic calculations of jet engines in real time, \texttt{JetBench} serves as an application benchmark that not only simulates a realistic workload but also measures the time required to complete these calculations with different thread counts. Its primary objective is to assess the multi-core capabilities of CPUs. The workload generated by \texttt{JetBench} predominantly consists of Arithmetic Logic Unit (ALU) centric operations, including integer and double precision multiplication, addition, and division. These operations facilitate the computation of critical mathematical functions such as exponentiation, square roots, and conversions including the calculation of pi and degree-to-radian conversions. The software exhibits a significant parallel portion, constituting 88.6\% of its total operations. \texttt{JetBench} is developed in the \texttt{C} programming language and utilizes the \texttt{OpenMP} library to enable multi-threading, ensuring both efficiency and scalability. Furthermore, \texttt{JetBench} prioritizes portability, allowing it to assess the performance of a wide spectrum of systems, from low-end to high-end. This design philosophy ensures the avoidance of system-specific libraries or timers, underlining the software’s broad applicability and utility in performance evaluation across diverse computing environments.

The first paper\cite{JetBench_paper} provides valuable insights into the foundational aspects of this application benchmark, though it is not without its limitations. For instance, the application encompasses a limited scope of computations, a decision made to ensure compatibility with lower-end CPUs. However, this constraint may render the application seemingly inadequate for testing on more advanced CPUs. Additionally, the omission of input/output operations was a deliberate choice to enhance the application's portability, albeit at the expense of being unable to assess the I/O capabilities of the target platform. Despite these limitations, the benchmark's strength lies in its portability and its efficacy in evaluating the multi-core performance across a spectrum of computing systems, from low to high-end. These attributes deem the application an apt selection for this project, which seeks to explore and develop multi-threading capabilities for both desktop and embedded platforms.

To address the identified limitations of the \texttt{JetBench} software, a novel solution was devised using diverse programming languages. This innovation was detailed in a publication titled ``Using JetBench to Evaluate the Efficiency of Multiprocessor Support for Parallel Processing,'' authored by HaiTao Mei and Andy Wellings. The paper, released in 2014, was part of the proceedings of a notable conference\cite{mpbenchmark_paper}.

In their research, the authors of \texttt{mpbenchmark}\cite{mpbenchmark_paper} adopted an object-oriented programming (OOP) approach for C\# and Java implementations. They standardized result collection by executing the application 30 times with a varying number of threads, employing a specific Linux command to control CPU core usage. Results were collected using a desktop CPU and \texttt{Simics}(a virtual platform to simulate a high end 128-core CPU).  This methodology is in line with standard practices for benchmark data collection.

The authors of \texttt{mpbenchmark}\cite{mpbenchmark_paper} highlighted several design flaws in the original \texttt{JetBench} software\cite{JetBench_paper}, including the excessive use of shared variables leading to inaccurate performance results, race conditions from variables shared across threads, and erroneous benchmark output data printing. To remedy these issues, they restructured the \texttt{JetBench} code and introduced implementations in \texttt{Ada}, \texttt{C\#}, and \texttt{Java}. Notably, the compiled languages (\texttt{C} and \texttt{Ada}) demonstrated superior performance. Additionally, the impact of virtual cores enabled by simultaneous multi-threading (SMT) was observed to vary inconsistently across different programming languages.

In the second paper\cite{mpbenchmark_paper} several limitations were found. Firstly, the data collection, based on just 30 runs, may be insufficient, especially for applications with minimal execution times on high-end CPUs. Additionally, the research focused solely on desktop CPUs and a system simulator mimicking a high-end 128-core CPU, excluding experiments on embedded or lower-end CPUs. Furthermore, given the superior performance of compiled languages like C and Ada, further research could beneficially explore comparisons with \texttt{C++}, another compiled language often regarded as superior to both C and Ada in certain contexts. This paper lays a solid foundation for our project, where its limitations present opportunities for further investigation, and its methodological approaches offer a model for emulation.

\section{Objective 2: \texttt{MobileNet}}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

\texttt{MobileNet} is a specialized machine learning algorithm that falls under the category of convolutional neural networks (CNNs). Developed specifically for mobile and embedded vision applications, \texttt{MobileNet} is discussed in detail in a 2017 paper by Cornell University titled ``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"\cite{mobilenet_paper}. 

This paper\cite{mobilenet_paper} delves into the mathematical foundations required to construct the \texttt{MobileNet}, which are considered beyond the scope of our project. To summarize, the \texttt{MobileNet} architecture employs depth-wise separable convolutions that enhance memory efficiency and leverage optimized numerical linear algebra algorithms to reduce latency.

\texttt{MobileNet} introduces two crucial hyper-parameters: the width and resolution multiplier. These allow for adjustments to the model's size and computational requirements, with smaller \texttt{MobileNet} models typically showing slightly reduced accuracy compared to their larger counterparts. In comparative analyses with other renowned models like \texttt{GoogleNet}, \texttt{VGG 16}, and \texttt{FaceNet}; \texttt{MobileNet} offers comparable accuracy levels.

Despite its advantages of reduced size and latency, making it ideal for mobile and embedded devices, the paper\cite{mobilenet_paper} does not address the use of parallel computing techniques, presenting an opportunity for further exploration of its source code to enhance performance through parallelism.

For the practical component of our project, a \texttt{C++} implementation of \texttt{MobileNet} was chosen, sourced from a public repository on the \texttt{GitHub}\cite{mobilenet_repo}. This particular implementation was part of a computer science competition\cite{mobilenet_competition}, challenging participants to classify images into 20 predefined classes using provided training, validation, and testing data, with submissions made in a \texttt{.zip} file format.

The \texttt{C++} implementation comprises several header and source files but lacks build software or instructions, which poses a significant initial hurdle. Moreover, the implementation features a method for testing custom images against 12 predefined classes. However, these class names are in Chinese, necessitating translation into English for broader accessibility. Setting up and running the project before making any enhancements will be challenging. Nevertheless, this \texttt{C++} implementation presents a valuable opportunity to apply parallelisation techniques to improve the efficiency of the widely recognized \texttt{MobileNet} algorithm.

\section{Objective 3: \texttt{DeBaTE-FI} platform}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

The \texttt{DeBaTE-FI} platform, developed by PhD student Alex Henneman under the supervision of Dr. Luciano Ost at Loughborough University, is a GUI application detailed in the publication ``DeBaTE-FI: A Debugger-Based Fault Injector Infrastructure for IoT Soft Error Reliability Assessment"\cite{debate_fi_publication}.

\texttt{DeBaTE-FI} was engineered to test embedded devices for soft errors—random errors in processors caused by background radiation, which can be particularly harmful in safety-critical systems. Testing methods for soft errors range from high-level analytical models, which are easier to implement but less accurate, to costly full radiation tests, which offer higher precision. \texttt{DeBaTE-FI} seeks to emulate the latter's accuracy by simulating soft errors in embedded devices. The platform utilized \texttt{Raspberry Pi 4} and \texttt{Rock 4C} to conduct tests on \texttt{STM32F7} microcontrollers (MCUs). The research involved running a neural network binary on these MCUs, measuring the execution time across various testing scenarios, and using different numbers of MCUs to evaluate speed and efficiency. Additionally, the results from \texttt{DeBaTE-FI} were compared with those from simulation software to assess accuracy, and \texttt{OpenOCD} was used for MCU communication.

The authors observed that increasing the number of MCUs reduced overall runtime. They also discovered that while multithreading in Python did not improve performance, multiprocessing—which enables parallel processing—did enhance performance significantly. The study confirmed that \texttt{DeBaTE-FI} provides greater accuracy than the high-level simulation software they aimed to surpass. Challenges arose when using \texttt{Raspberry Pi 4}, particularly with USB controller limitations that restricted the connection to no more than 8 MCUs, prompting a switch to \texttt{Rock 4C}, which supported up to 30 MCUs but also reached full CPU utilization under this load.

Despite the innovative approach of \texttt{DeBaTE-FI}, the platform is hindered by several significant limitations. The primary issue is its lengthy runtime, which can extend to several minutes, suggesting the presence of performance bottlenecks. Moreover, the performance on embedded Linux devices, such as \texttt{Rock 4C}, is modest, highlighting the need for optimization across both desktop and embedded Linux devices (or single-board computers). Additionally, the utilization of \texttt{OpenOCD} for MCU communication presents an opportunity to enhance efficiency and expedite processes. The central goal of our project is to tackle these issues by improving the runtime efficiency and scalability of \texttt{DeBaTE-FI}.

\section{Cross platform build systems}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

A 2008 paper published by Technical University of Munich titled  ``A Cross Platform Development Workflow for C/C++ Application"\cite{cmake_publication_1} underscores the efficacy of \texttt{CMake} in addressing significant challenges in cross-platform project management, particularly those stemming from the constraints of build systems and source code management practices. This is highlighted despite the standardization of \texttt{C} and \texttt{C++} by \texttt{ANSI} and \texttt{ISO}, and the availability of the \texttt{C} library and \texttt{STL}. Given our project's focus on constructing \texttt{C++} applications and the need to automate build processes across multiple operating systems such as Linux, macOS, and Windows, \texttt{CMake} emerges as an suitable choice. 

The authors demonstrate \texttt{CMake}'s versatility in allowing developers to maintain their preferred development environments across various platforms without the need to synchronize platform-specific project files. By integrating \texttt{CMake} into prominent projects like \texttt{KDE4}, and utilizing it to develop an \texttt{OpenGL} application and a platform-independent camera interface, the paper advocates for a workflow that significantly simplifies the development of cross-platform applications.

An essential feature of a build system, as highlighted, is its ability to support the configuration step to locate necessary header and library files of external packages. The text describes how CMake aids this process by offering numerous pre-written configuration scripts for popular libraries. It also mentions that developers can write their own reusable configuration scripts to tailor the setup to their specific needs, which is crucial for managing dependencies in software projects. The text discusses the implementation of \texttt{qiew}, a platform-independent \texttt{VRML} viewer using the \texttt{Qt} toolkit and \texttt{Coin3D}. This case exemplifies a specific application of cross-platform development practices to ensure consistent user experiences across different operating systems.

CMake is characterized by a relatively gentle learning curve compared to tools like GNU Autotools, enhancing its accessibility to software engineers. This ease of integration into existing development processes minimizes the need for extensive training, thus lowering the barriers to its adoption. A pertinent example is provided by the Technische Universität München, where diverse research groups—specifically in vision and robotics—operate on different systems. This scenario exemplifies the necessity and efficacy of \texttt{CMake} in connecting disparate development environments, illustrating its ability to synchronize work across multiple platforms. This functionality was a crucial factor in its adoption at the university. Further underscoring its utility, \texttt{CMake} has been implemented in numerous cross-platform projects within the university and has been adopted by two spin-off companies. The tool's benefits are manifold, encompassing simplified project generation, effective synchronization, seamless configuration, adept dependency resolution, and streamlined deployment packaging. Its use in prominent projects like \texttt{KDE4} and \texttt{OpenSceneGraph} not only showcases its integration capabilities but also suggests its potential for broader recognition and use. These examples highlight \texttt{CMake}'s critical role as an indispensable tool in managing cross-platform development projects efficiently.

While the paper thoroughly discusses \texttt{CMake}'s advantages, it does not sufficiently address potential limitations or challenges \texttt{CMake} could present, such as configuration overheads or limitations in handling more complex scenarios. Including comparisons with contemporary tools could enhance the analysis by highlighting \texttt{CMake}'s performance relative to its peers. Moreover, considering the rapid evolution of technology and software development practices, relying on a paper from 2008 presents certain risks. The software landscape has undergone significant transformations, with new programming languages and updated standards emerging. Development tools and environments have evolved, introducing functionalities that might surpass \texttt{CMake}'s capabilities or more effectively address its deficiencies. Additionally, the integration of modern DevOps practices and advanced \texttt{CI/CD} pipelines might render some of the methodologies discussed as outdated or less efficient in the current context. While the paper offers valuable insights into \texttt{CMake}'s role in cross-platform development, understanding the contemporary context of software engineering practices and toolchains is crucial to fully appreciate its current relevance and applicability.

Addressing the previous paper's limitations and potentially out of date content more research was conducted to find more recent publications around the use of build automation software. A recognised publication titled ``Large Scale Software Building with CMake in ATLAS" obtained from the ``Journal of Physics: Conference Series" published in 2017\cite{cmake_publication_2} discusses the use of build automation software for \texttt{C++} application in an industrial context. 

The discussion in the text outlines the use of \texttt{CMake} as a robust methodology for managing complex, multi-platform software projects. Key techniques include structuring projects using \texttt{CMake}'s subdirectory concept to mimic the package structure from the \texttt{CMT} build system, with a master \texttt{CMakeLists.txt} file that centralizes configuration. This setup streamlines project and package management through \texttt{Atlas}-specific functions and macros, facilitates the inclusion of packages via \texttt{add\_subdirectory} calls, and employs helper targets for efficient building and testing. Additionally, exported targets ensure modularity by prefixing libraries with the project name, scripts generated for the runtime environment address external dependencies, and custom scripts make the project files relocatable, enhancing the system’s flexibility and operational consistency across different environments.

The major findings related to the theme of migrating the ATLAS offline software to a new build system using \texttt{CMake} focus on enhancing efficiency and streamlining the build and installation processes. \texttt{CMake} was chosen for its ability to efficiently parallelize the build of independent components, which has made effective use of resources on build machines, as evidenced by monitoring systems. Although the transition to \texttt{CMake} initially only slightly reduced build times, the consolidation of projects into a single repository named Athena has significantly improved performance, achieving the goal to complete builds in approximately three hours. This change aligns \texttt{ATLAS} software development practices more closely with common software development procedures, particularly with the shift towards using \texttt{Git} for version control. Overall, the new system not only meets the performance and feature requirements of the experiment but is also expected to continue through \texttt{LHC's Run 3} and beyond due to its success in the initial stages and its validation for the 2017 data taking.

The critical evaluation of the ATLAS experiment's shift to \texttt{CMake} for building its offline software identifies several strengths and weaknesses. Strengths include improved build efficiency, with the potential to significantly reduce build times by consolidating projects into a single entity called Athena, and enhanced community support due to the adoption of \texttt{CMake} over the previous \texttt{CMT} system. This transition aligns ATLAS with common software development practices through the integration of \texttt{CMake} and \texttt{Git}, facilitating easier updates and collaboration. However, weaknesses are noted in complex dependency management and the modest improvements in build times during the transition, suggesting initial inefficiencies. Gaps such as the lack of detailed performance comparisons and discussions on long-term scalability raise concerns about the system's future adaptability and optimization. Overall, the transition has modernized ATLAS’s build processes, but continuous evaluation and refinement are necessary to maximize the benefits of \texttt{CMake}.

Conducting research on build automation software for \texttt{C++} projects\cite{cmake_publication_1}\cite{cmake_publication_2} outline the \texttt{CMake} software as a useful tool that is industry standard and can be used for this project. 

\section{\texttt{C++} guidelines and best practices}
% Brief overview and relevance to the project 
% Discussion of methodologies used in the studies.
% Summary of major findings related to the theme.
% Critical evaluation of the strengths, weaknesses, and gaps.

Multi-threading in C++ is a complex topic therefore in order to adhere to \texttt{C++} core guidelines,  textbook titled ``\texttt{C++} concurrency in Action" written by Anthony William was researched\cite{c++_concurrency_in_action}. This book is regarded by many as the gold standard for multi-threading concepts in modern \texttt{C++}, as discussed by the community on stack overflow\cite{c++_books_stackoverflow}. The second edition of this book has been updated to include features the \texttt{C++17} standard.

The book\cite{c++_concurrency_in_action} thoroughly examines the methodologies for developing concurrent and multithreaded applications with \texttt{C++}. This edition not only reviews the foundational principles from \texttt{C++11} and \texttt{C++14}, but also incorporates advancements from \texttt{C++17}, providing an extensive overview of modern \texttt{C++} concurrency features. Williams methodically explores various strategies, including lock-based and lock-free programming, as well as the development of concurrent data structures and algorithms. Each technique is clearly explained, focusing on practical implementation and the effective use of the language's concurrency capabilities. The book also addresses the challenges of concurrency with detailed examples and solutions, making it an essential resource for developers aiming to utilize the full potential of multicore processing with \texttt{C++}.

Strengths of the book\cite{c++_books_stackoverflow} include comprehensive explanations and practical examples that are accessible even to programmers with a basic understanding of \texttt{C++}, making complex concurrency concepts easier to grasp. Williams meticulously explores both fundamental and advanced concurrency techniques, accompanied by a methodical presentation of best practices and common pitfalls, which helps developers build robust, high-performance applications. However, the book's weaknesses include its dense technical content, potentially overwhelming for beginners without a solid \texttt{C++} background or those new to concurrency. While it thoroughly addresses \texttt{C++17}, those seeking insights into the latest concurrency features of \texttt{C++20} might find the coverage lacking, highlighting a gap in an otherwise excellent resource. This gap points to an area for potential future updates or editions as \texttt{C++} continues to evolve. Overall, the book remains an invaluable resource for both experienced and intermediate \texttt{C++} developers aiming to deepen their understanding of concurrent programming.

Another valuable study, titled ``Multi-Threaded Parallel I/O for OpenMP Applications," was published in the International Journal of Parallel Programming in 2014\cite{openmp_usage_hpc}. This research is pertinent to our project as various libraries exhibit unique strengths and weaknesses, and selecting the appropriate library is crucial for achieving optimal performance.

The methodologies discussed in the study focus on optimizing multi-threaded parallel I/O operations within the \texttt{OpenMP} framework, primarily through a parallel I/O library integrated into the \texttt{OpenUH} compiler. Key components include the use of \texttt{omp\_file\_read\_all} for collective data reading within a parallel region, and advanced strategies such as merging small I/O requests into larger ones and splitting large requests among multiple threads to enhance efficiency. The system also incorporates a contiguity analyzer and a work manager to optimize data handling and thread utilization. Additionally, adaptive techniques are applied to accommodate different file system behaviors, with specific optimizations like the use of the Parallel Log-structured File System (\texttt{PLFS}) to optimize performance on systems like \texttt{Lustre}. These methodologies collectively aim to reduce operational latency and improve throughput in high-performance computing environments, leveraging collective I/O operations and strategic thread management to maximize resource utilization.

The major findings from the research on parallel I/O interfaces for \texttt{OpenMP} applications reveal significant advancements in I/O operations performance, particularly for write operations. The introduction of these interfaces allows multiple user-level threads to enhance functionality and performance, achieving write speeds nearly equivalent to each thread writing to separate files—the upper limit within a single node. This approach notably circumvents the laborious merging step required by the one-file-per-thread method, resulting in potentially dramatic performance improvements. However, read operations did not show similar enhancement potential, primarily because file systems typically do not employ locking mechanisms for reads as they do for writes. Performance gains from the new I/O operations appear to plateau beyond two to four threads, indicating a limitation in scalability to very high numbers of threads, although these improvements are still substantial. The study also considers the integration of these \texttt{OpenMP} I/O routines with hybrid \texttt{MPI/OpenMP} applications, suggesting that while concurrent use of \texttt{MPI I/O} and \texttt{OpenMP I/O} is not feasible due to domain partitioning in \texttt{MPI}, optimizations from \texttt{OpenMP I/O} could potentially enhance internal aggregator processes in \texttt{MPI I/O} systems like \texttt{ROMIO} and \texttt{OMPIO}. Further, the paper outlines future directions for this research, including updating interface specifications to leverage recent \texttt{OpenMP} developments and enhancing the parallel I/O library's efficiency through data locality optimizations and resource amalgamation.

The paper\cite{openmp_usage_hpc} provides a comprehensive evaluation of newly developed parallel I/O interfaces tailored for \texttt{OpenMP} applications, addressing a significant gap in support for shared-memory programming models. The research successfully demonstrates substantial performance enhancements over traditional sequential I/O operations through the implementation of these interfaces within the \texttt{OpenUH} compiler. The study thoroughly discusses the design rationale, interface specifications, and several optimizations, showcasing a notable improvement in I/O performance across various file systems and benchmark scenarios. Comparisons are made with existing \texttt{MPI} I/O functions, illustrating the new interfaces' superiority in certain contexts. The future directions of this research are promising, focusing on refining the interfaces to leverage recent \texttt{OpenMP} advancements and to optimize the parallel I/O library to exploit data locality in \texttt{NUMA} architectures more effectively. The authors also plan to enhance the library's performance further by integrating multiple storage resources and expanding its applicability to more real-world applications and diverse computing environments. This forward-looking approach indicates a robust framework for ongoing improvement that could significantly impact high-performance computing related to data-intensive applications.

The project aims to emulate the techniques and software design highlighted in the book\cite{c++_concurrency_in_action} to ensure the produced software adheres to the best practices. Another library, \texttt{OpenMP}, which is widely used in high-performance computing and parallel I/O interfaces\cite{openmp_usage_hpc}, offers a straightforward method for implementing multi-threading in both \texttt{C} and \texttt{C++}. This library is particularly useful for our project as it provides an alternative to the \texttt{std::thread} library in \texttt{C++}. 

\section{Literature review conclusion}

Research on the \texttt{JetBench} software has been identified as essential, as documented in the \texttt{mpbenchmark} publication\cite{mpbenchmark_paper}, which tracks its evolution and benchmarks performance across various programming languages. Analysis of both the \texttt{mpbenchmark} and \texttt{JetBench} papers\cite{JetBench_paper}\cite{mpbenchmark_paper} reveals substantial advancements, providing a robust foundation for this project to develop a more optimized \texttt{C++} solution. This initiative, guided by best practices from the renowned \texttt{C++ Concurrency in Action}\cite{c++_concurrency_in_action}—a resource recommended on Stack Overflow\cite{c++_books_stackoverflow}, builds upon existing work without redundancy.

For the second objective, understanding of the \texttt{MobileNet} algorithm was enhanced through its foundational publication\cite{mobilenet_paper}. The project began with a \texttt{C++} implementation of \texttt{MobileNet}, sourced from an open GitHub repository\cite{mobilenet_repo}, to analyse and enhance the code through parallelisation. Key resources for this enhancement include the \texttt{C++} concurrency textbook\cite{c++_concurrency_in_action} and various multi-threading libraries\cite{openmp_usage_hpc}, which are crucial for optimizing performance.

The first two objectives necessitate the development of cross-platform \texttt{C++} applications, for which \texttt{CMake} is suitable for this project. Investigations into \texttt{CMake}'s application across multiple platforms have been informed by pertinent literature\cite{cmake_publication_1}\cite{cmake_publication_2}. This research is essential to streamline the build and compilation processes, ensuring they meet industry standards.

The third objective addresses a unique challenge: enhancing the \texttt{DeBaTE-FI} platform, developed by Loughborough University’s PhD students and faculty\cite{debate_fi_publication}. A thorough review of its publication was critical to comprehend its design and functionality. The project will leverage the existing features of \texttt{DeBaTE-FI}, aiming to refine its application and benchmark against the original performance metrics. 


